"""
COMPREHENSIVE USAGE GUIDE
Federated Learning Attack Simulation with Flower Framework
"""

================================================================================
QUICK START GUIDE
================================================================================

1. VERIFY ENVIRONMENT
   
   Ensure your virtual environment is activated:
   - Windows PowerShell:
     .\flower_env\Scripts\Activate.ps1
   
   - Linux/Mac:
     source flower_env/bin/activate

2. INSTALL DEPENDENCIES (if not already installed)
   
   pip install -r requirements.txt
   
   Or manually:
   pip install "flwr[simulation]>=1.23.0" "flwr-datasets[vision]>=0.5.0"
   pip install torch==2.7.1 torchvision==0.22.1
   pip install matplotlib scikit-learn

3. TEST YOUR SETUP (RECOMMENDED)
   
   Run a quick 10-round experiment to verify everything works:
   
   python test_setup.py
   
   This should complete in 5-10 minutes and create:
   - results/metrics_FedAvg_iid_0attackers.json
   - results/final_model_FedAvg_iid_0attackers.pt

4. RUN ALL EXPERIMENTS
   
   Once the test succeeds, run the full experiment suite:
   
   python run_experiments.py
   
   ⚠️  This will run 12 experiments and take 2-4 hours!
   
   Experiments that will be run:
   ✓ FedAvg + IID + Baseline
   ✓ FedAvg + IID + 1 Attacker
   ✓ FedAvg + IID + 2 Attackers
   ✓ FedAvg + Non-IID + Baseline
   ✓ FedAvg + Non-IID + 1 Attacker
   ✓ FedAvg + Non-IID + 2 Attackers
   ✓ FedProx + IID + Baseline
   ✓ FedProx + IID + 1 Attacker
   ✓ FedProx + IID + 2 Attackers
   ✓ FedProx + Non-IID + Baseline
   ✓ FedProx + Non-IID + 1 Attacker
   ✓ FedProx + Non-IID + 2 Attackers

5. ANALYZE RESULTS
   
   After experiments complete, generate plots and analysis:
   
   python analyze_results.py
   
   This creates:
   - plots/ directory with comparison visualizations
   - analysis_report.txt with detailed metrics

================================================================================
UNDERSTANDING THE IMPLEMENTATION
================================================================================

ATTACK MECHANISM: Label Flipping
- Malicious clients receive correct data but flip labels randomly
- Attack is applied during training in task.py
- Attacker clients are specified by ID (0-4 for 5 clients)

DATA DISTRIBUTIONS:
- IID: Data evenly distributed across clients using IidPartitioner
- Non-IID: Heterogeneous distribution using DirichletPartitioner (α=0.5)
  Lower alpha = more non-IID

AGGREGATION STRATEGIES:
- FedAvg: Standard weighted averaging of client models
- FedProx: Adds proximal term (μ=0.1) to prevent client drift

METRICS TRACKED:
- Accuracy: Overall classification accuracy
- F1 Score: Macro-averaged F1 (handles class imbalance)
- Kappa: Cohen's Kappa (agreement measure)
- ROC-AUC: Multi-class area under ROC curve

================================================================================
RUNNING INDIVIDUAL EXPERIMENTS
================================================================================

If you want to run specific experiments instead of the full suite:

BASELINE (NO ATTACK):
flwr run . --run-config num-server-rounds=50 use-iid=true strategy=FedAvg attacker-ids=[]

1 ATTACKER (CLIENT 0):
flwr run . --run-config num-server-rounds=50 use-iid=true strategy=FedAvg attacker-ids=[0] attack-type=label_flipping

2 ATTACKERS (CLIENTS 0 AND 1):
flwr run . --run-config num-server-rounds=50 use-iid=true strategy=FedAvg attacker-ids=[0,1] attack-type=label_flipping

NON-IID DATA:
flwr run . --run-config num-server-rounds=50 use-iid=false strategy=FedAvg attacker-ids=[0]

FEDPROX STRATEGY:
flwr run . --run-config num-server-rounds=50 strategy=FedProx attacker-ids=[0] proximal-mu=0.1

================================================================================
UNDERSTANDING THE RESULTS
================================================================================

RESULTS FILES (in results/ directory):

Each experiment generates a JSON file with structure:
{
  "experiment_config": {
    "strategy": "FedAvg",
    "data_distribution": "iid",
    "num_attackers": 1,
    ...
  },
  "evaluate_metrics_serverapp": {
    "1": {"accuracy": 0.42, "f1": 0.40, ...},
    "2": {"accuracy": 0.51, "f1": 0.48, ...},
    ...
  }
}

The "evaluate_metrics_serverapp" section contains per-round metrics
evaluated on the centralized test set (most reliable).

PLOTS (in plots/ directory after running analyze_results.py):

1. Strategy-specific comparisons:
   - FedAvg_iid_accuracy_comparison.png
   - FedAvg_non_iid_f1_comparison.png
   - etc.
   
   These show how attackers impact a specific strategy/data combination.

2. Cross-strategy comparisons:
   - strategy_comparison_iid_Baseline_accuracy.png
   - strategy_comparison_non_iid_2_Attackers_f1.png
   - etc.
   
   These compare FedAvg vs FedProx under same conditions.

3. Summary dashboard:
   - summary_dashboard.png
   
   Shows final metrics for all 12 experiments side-by-side.

================================================================================
ANALYSIS QUESTIONS TO ANSWER
================================================================================

PART 2.1: FedAvg Analysis

1. IID Data:
   Q: How does accuracy degrade with 1 vs 2 attackers?
   → Compare baseline vs 1 attacker vs 2 attackers plots
   
   Q: Which metric shows strongest degradation?
   → Look at relative changes in Accuracy, F1, Kappa, ROC

2. Non-IID Data:
   Q: Are attacks more effective on non-IID data?
   → Compare IID vs non-IID plots for same attacker count
   
   Q: Does data heterogeneity amplify attack impact?
   → Check if performance gaps are larger in non-IID scenarios

PART 2.2: FedProx vs FedAvg

1. Robustness Comparison:
   Q: Is FedProx more resilient to attacks than FedAvg?
   → Compare cross-strategy plots
   
   Q: Does proximal term help mitigate poisoning?
   → Check if FedProx maintains higher metrics under attack

2. Data Distribution Interaction:
   Q: Does FedProx provide better protection on non-IID data?
   → Compare FedProx advantage in IID vs non-IID scenarios

FINAL CONCLUSIONS:
- Vulnerability assessment: How many attackers significantly harm FL?
- Distribution impact: IID vs non-IID vulnerability
- Defense effectiveness: FedProx vs FedAvg robustness
- Metric reliability: Which metrics best indicate attack presence?

================================================================================
CUSTOMIZATION OPTIONS
================================================================================

CHANGE ATTACK TYPE:
Edit flower2025/task.py in the train() function:
- Label flipping: labels = torch.randint(0, 10, labels.shape, device=device)
- Targeted flipping: labels = (labels + 1) % 10
- Gradient inversion: Negate gradients before backprop
- Model poisoning: Inject malicious weights

ADJUST NON-IID SEVERITY:
Edit flower2025/task.py in load_data():
- More non-IID: alpha=0.1 (extreme heterogeneity)
- Less non-IID: alpha=1.0 (moderate heterogeneity)
- Default: alpha=0.5

CHANGE NUMBER OF CLIENTS:
Edit pyproject.toml:
[tool.flwr.federations.local-simulation]
options.num-supernodes = 10  # Instead of 5

MODIFY TRAINING PARAMETERS:
Edit pyproject.toml:
[tool.flwr.app.config]
num-server-rounds = 100    # More rounds
local-epochs = 2           # More local training
lr = 0.001                 # Different learning rate

================================================================================
TROUBLESHOOTING
================================================================================

ISSUE: "ModuleNotFoundError: No module named 'flwr'"
SOLUTION: Activate virtual environment and install dependencies
  .\flower_env\Scripts\Activate.ps1
  pip install -r requirements.txt

ISSUE: "Dataset download timeout"
SOLUTION: 
  - Check internet connection
  - Retry (dataset caches after first download)
  - Manually download CIFAR-10 if needed

ISSUE: "CUDA out of memory"
SOLUTION:
  - Force CPU usage: Set CUDA_VISIBLE_DEVICES="" in environment
  - Reduce batch size in task.py (default: 32 → try 16)
  - Close other GPU applications

ISSUE: "Experiments taking too long"
SOLUTION:
  - Run with fewer rounds for testing:
    python test_setup.py (10 rounds)
  - Run experiments individually instead of full suite
  - Use GPU if available (significant speedup)

ISSUE: "Plots not generating"
SOLUTION:
  - Ensure matplotlib is installed: pip install matplotlib
  - Check that results/ directory has JSON files
  - Run analyze_results.py from project root directory

ISSUE: "Global object 'fds' prevents non-IID switching"
SOLUTION:
  - This is expected - restart Python process between IID/non-IID runs
  - The run_experiments.py script handles this automatically
  - If running manually, restart kernel/terminal between experiments

================================================================================
EXPECTED RESULTS (TYPICAL PERFORMANCE)
================================================================================

BASELINE (No attackers):
- Final Accuracy: 0.65-0.75
- Final F1 Score: 0.63-0.73
- Final Kappa: 0.60-0.70
- Final ROC-AUC: 0.85-0.92

WITH 1 ATTACKER (20% malicious):
- Accuracy drops by ~5-15%
- F1 and Kappa show similar degradation
- ROC-AUC slightly more robust

WITH 2 ATTACKERS (40% malicious):
- Accuracy drops by ~15-30%
- Severe degradation across all metrics
- Model may fail to converge properly

NON-IID EFFECT:
- Baseline performance ~5-10% lower than IID
- Attack impact amplified by ~5% additional degradation
- Higher variance across rounds

FEDPROX BENEFIT:
- Similar baseline performance to FedAvg
- Approximately 3-8% better resilience under attack
- More consistent convergence on non-IID data

================================================================================
TIMELINE AND RESOURCE ESTIMATES
================================================================================

COMPUTATION TIME (CPU: Intel i7, 16GB RAM):
- Single experiment (50 rounds): 10-15 minutes
- Full suite (12 experiments): 2-4 hours
- Test experiment (10 rounds): 3-5 minutes

COMPUTATION TIME (GPU: NVIDIA RTX 3070):
- Single experiment: 5-8 minutes
- Full suite: 1-2 hours
- Test experiment: 1-2 minutes

DISK SPACE:
- Results per experiment: ~200KB JSON + ~50MB model
- Total for 12 experiments: ~3MB JSON + ~600MB models
- Plots: ~15MB
- Dataset cache (first download): ~170MB

MEMORY REQUIREMENTS:
- Minimum: 8GB RAM
- Recommended: 16GB RAM
- Peak usage: ~4-6GB per experiment

================================================================================
GETTING HELP
================================================================================

DOCUMENTATION:
- Flower Framework: https://flower.ai/docs/
- Flower Datasets: https://flower.ai/docs/datasets/
- PyTorch: https://pytorch.org/docs/

PROJECT FILES:
- README_ATTACK_SIMULATION.md: Detailed project documentation
- This file: Step-by-step usage guide
- pyproject.toml: Configuration reference

DEBUGGING:
1. Check terminal output for error messages
2. Verify virtual environment is activated
3. Ensure all dependencies are installed
4. Try test_setup.py first before full experiments
5. Check results/ directory for partial results

================================================================================
SUBMISSION CHECKLIST
================================================================================

For Part 2.1 and 2.2, ensure you have:

□ Run all 12 experiments (6 for Part 2.1, 6 for Part 2.2)
□ Generated plots for all scenarios
□ Created analysis report with final metrics
□ Documented observations about:
  - Attack impact on IID vs non-IID data
  - Differences between 1 and 2 attackers
  - FedAvg vs FedProx robustness
  - Vulnerability conclusions
□ Included code files:
  - flower2025/task.py
  - flower2025/client_app.py
  - flower2025/server_app.py
  - run_experiments.py
  - analyze_results.py
□ Included results:
  - All JSON files from results/
  - All plots from plots/
  - analysis_report.txt

================================================================================
