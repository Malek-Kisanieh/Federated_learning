\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

% Title information
\title{DV2607 - Adversarial Input Attack\\Inlämningsuppgift Del 2}
\author{Samir Akhalil \\ saaa21@student.bth.se \and Mhd Malek Kisaniehj \\ mhki22@student.bth.se}
\date{\today}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Del 1: Centraliserad Machine Learning}

\subsection{Beskrivning av adversarial input attacker}

Adversarial input attacker är en typ av säkerhetsattack mot maskininlärningsmodeller där små, nästan omärkliga förändringar görs i indata för att lura modellen att göra felaktiga prediktioner. Dessa förändringar, kallade \textit{perturbationer}, är designade för att vara så små att de inte kan uppfattas av det mänskliga ögat, men de är tillräckligt stora för att påverka modellens beslut dramatiskt.

Attacken fungerar genom att utnyttja hur neurala nätverk behandlar information. Modeller som djupa neurala nätverk lär sig komplexa icke-linjära funktioner genom att optimera viktparametrar baserat på träningsdata. Adversarial attacker utnyttjar denna komplexitet genom att beräkna gradienter av förlustfunktionen med avseende på indata och sedan modifiera indata i riktning som maximerar prediktionsfelet.

I vårt fall användes en \textit{targeted attack}, vilket betyder att målet inte bara är att få modellen att göra fel, utan att få den att klassificera bilden som en specifik, förutbestämd klass (traktor istället för koala).


\subsection{Implementation av er attack}

Vi implementerade en \textbf{Basic Iterative Method (BIM)} attack, även känd som I-FGSM (Iterative Fast Gradient Sign Method). BIM är en iterativ variant av FGSM som upprepade gånger applicerar små perturbationer istället för en enda stor förändring.

\textbf{Hur BIM fungerar:}
\begin{enumerate}
    \item Börja med originalbilden $x_0$
    \item För varje iteration $t = 1, 2, \ldots, N$:
    \begin{itemize}
        \item Beräkna gradienten av förlustfunktionen med avseende på indata
        \item Ta ett litet steg $\epsilon_{\text{step}}$ i riktning som ökar förlusten för målklassen
        \item Klipp resultatet så att total perturbation inte överskrider $\epsilon_{\text{max}}$
    \end{itemize}
    \item Returnera den modifierade bilden $x_{\text{adv}}$
\end{enumerate}

Matematiskt kan detta uttryckas som:
\[x_{t+1} = \text{Clip}_{x_0, \epsilon}\left(x_t + \epsilon_{\text{step}} \cdot \text{sign}(\nabla_x L(x_t, y_{\text{target}}))\right)\]

där $L$ är förlustfunktionen och $y_{\text{target}}$ är målklassen (traktor).

\textbf{Bibliotek som användes:}
\begin{itemize}
    \item \texttt{TensorFlow/Keras}: För att ladda ResNet50-modellen och göra prediktioner
    \item \texttt{Adversarial Robustness Toolbox (ART)}: För att implementera BIM-attacken genom klassen \texttt{BasicIterativeMethod}
    \item \texttt{NumPy}: För array-manipulering och numeriska beräkningar
    \item \texttt{Matplotlib}: För visualisering av bilder och perturbationer
\end{itemize}

Våra hyperparametrar var: $\epsilon_{\text{max}} = 8.0$, $\epsilon_{\text{step}} = 2.0$, och $\text{max\_iter} = 40$. Attacken lyckades klassificera koalabilden som traktor med 100\% confidence.


\subsection{Säkerhetsåtgärder}

Vi valde \textbf{Gaussian Blur} som skyddsåtgärd mot BIM-attacken. Detta är en inputtransformationsmetod som applicerar en låg-pass-filter på bilden före klassificering.

\textbf{Motivering för valet:}

\begin{enumerate}
    \item \textbf{Effektivitet mot högfrekventa perturbationer}: BIM-attacken lägger till små, högfrekventa störningar i bilden. Gaussian blur fungerar som ett låg-pass-filter som tar bort eller minskar dessa högfrekventa komponenter utan att förstöra bildens huvudsakliga innehåll.
    
    \item \textbf{Enkelhet och effektivitet}: Jämfört med mer komplexa metoder som adversarial training (som kräver oträning av hela modellen) eller defensive distillation, är Gaussian blur enkelt att implementera och kräver ingen modifiering av modellen själv.
    
    \item \textbf{Beräkningseffektivitet}: Blur-operationen är mycket snabb och kan appliceras i realtid, vilket gör den praktisk för produktionsmiljöer.
    
    \item \textbf{Bevarar bildkvalitet}: Med rätt radius (vi använde radius=1) kan blur ta bort adversarial perturbationer samtidigt som bilden fortfarande ser acceptabel ut för mänskliga ögon.
\end{enumerate}

\textbf{Alternativa metoder vi övervägde:}
\begin{itemize}
    \item \textit{Adversarial Training}: Träna modellen med adversarial exempel, men detta kräver oträning och stora datamängder
    \item \textit{Input Quantization}: Reducera bildupplösningen, men detta kan påverka legitima prediktioner negativt
    \item \textit{Defensive Distillation}: Träna en mjukare modell, men kräver tillgång till träningsdata och är beräkningsintensivt
\end{itemize}

Gaussian blur erbjuder den bästa balansen mellan effektivitet, enkelhet och prestanda för vårt specifika användningsfall.


\subsection{Implementation av skyddsåtgärder (frivilligt)}

Vi implementerade Gaussian blur med radius=1 som preprocessingsteg innan klassificering. Resultaten visar att skyddsåtgärden var mycket effektiv:

\textbf{Resultat:}
\begin{itemize}
    \item \textbf{Före skydd}: Den adversarial bilden klassificerades som \textit{tractor} med 100.0\% confidence
    \item \textbf{Efter skydd}: Den blurrade bilden klassificerades korrekt som \textit{koala} med 97\% confidence
\end{itemize}

\textbf{Analys:}

Gaussian blur lyckades helt neutralisera BIM-attacken. Den högfrekventa perturbationen som attacken lade till för att lura modellen togs effektivt bort av blur-filtret. Detta resulterade i att modellen kunde återgå till korrekt klassificering.

Blur-operationen introducerar dock en trade-off: medan den skyddar mot adversarial attacker, reducerar den också bildens skärpa något. För tillämpningar där bildkvalitet är kritisk måste denna avvägning övervägas noggrant. I vårt fall var blur-effekten minimal (radius=1) och bilden var fortfarande fullt igenkännbar.

Det är också viktigt att notera att detta skydd inte är universellt. Mer sofistikerade attacker (t.ex. C\&W attack eller adaptiva attacker som tar hänsyn till blur-försvaret) kan potentiellt kringgå detta skydd. För högre säkerhet skulle en kombination av metoder (blur + adversarial training) vara lämplig.


\section{Del 2: Federerad Machine Learning}

\subsection{Del 2.1: FedAvg med attacker}

\subsubsection{Beskrivning av federated learning scenario}

Federerat lärande (FL) är en distribuerad maskininlärningsmetod där flera klienter tränar en gemensam modell lokalt på sina egna data utan att dela rådata med en central server. I vårt scenario simulerade vi ett FL-system med följande konfiguration:

\begin{itemize}
    \item \textbf{Dataset}: CIFAR-10 (10 klasser av 32x32 färgbilder)
    \item \textbf{Antal klienter}: 5
    \item \textbf{Kommunikationsrundor}: 50
    \item \textbf{Aggregeringsstrategi}: FedAvg (Federated Averaging)
    \item \textbf{Modell}: Convolutional Neural Network (CNN) med 3 konvolutionslager
    \item \textbf{Datadistributioner}: IID (Independent and Identically Distributed) och Non-IID (Dirichlet distribution med $\alpha = 0.5$)
\end{itemize}

\textbf{FedAvg-algoritmen}:
\begin{enumerate}
    \item Server initierar en global modell med slumpmässiga vikter
    \item För varje kommunikationsrunda $t = 1, 2, \ldots, T$:
    \begin{itemize}
        \item Server skickar den aktuella globala modellen till alla klienter
        \item Varje klient $k$ tränar modellen lokalt på sin data och får uppdaterade vikter $w_k^t$
        \item Klienter skickar sina uppdaterade vikter tillbaka till servern
        \item Server aggregerar viktuppdateringarna genom viktad medelvärdesbildning:
        \[w^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_k^t\]
        där $n_k$ är antal samples hos klient $k$ och $n = \sum_{k=1}^{K} n_k$
    \end{itemize}
    \item Returnera den slutliga globala modellen
\end{enumerate}

\subsubsection{Implementation av label flipping attack}

Vi implementerade en \textbf{label flipping attack} där vissa klienter är malicious och manipulerar sina träningslabels. Specifikt:

\begin{itemize}
    \item \textbf{Attackmekanism}: Malicious klienter ersätter korrekta labels med slumpmässiga felaktiga labels under lokal träning
    \item \textbf{Attackare}: Vi testade med 0, 1, eller 2 malicious klienter av totalt 5 (0\%, 20\%, 40\%)
    \item \textbf{Malicious klient-ID}: Klient 3 (vid 1 attackerare) och klienter 3-4 (vid 2 attackerare)
\end{itemize}

Attacken implementerades i \texttt{client\_app.py} genom att modifiera träningsfunktionen:
\begin{verbatim}
if is_attacker:
    # Replace true labels with random labels
    labels = torch.randint(0, 10, labels.shape)
\end{verbatim}

\subsubsection{Experimentella resultat}

Vi utförde 6 experiment för FedAvg med olika kombinationer av datadistribution och antal attackerare:

\textbf{IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = 75.0\%, F1 = 74.3\%, Kappa = 0.722, ROC-AUC = 0.969
    \item \textbf{1 attackerare}: Accuracy = 68.6\%, F1 = 67.5\%, Kappa = 0.650, ROC-AUC = 0.948 (8.5\% degradering)
    \item \textbf{2 attackerare}: Accuracy = 56.4\%, F1 = 55.5\%, Kappa = 0.515, ROC-AUC = 0.893 (24.8\% degradering)
\end{itemize}

\textbf{Non-IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = 74.0\%, F1 = 61.7\%, Kappa = 0.686, ROC-AUC = 0.968
    \item \textbf{1 attackerare}: Accuracy = 68.2\%, F1 = 52.3\%, Kappa = 0.612, ROC-AUC = 0.933 (7.9\% degradering)
    \item \textbf{2 attackerare}: Accuracy = 42.4\%, F1 = 38.2\%, Kappa = 0.353, ROC-AUC = 0.869 (42.7\% degradering)
\end{itemize}

\textbf{Analys}:
\begin{itemize}
    \item Label flipping-attacken hade signifikant negativ påverkan på modellens prestanda
    \item Påverkan ökade icke-linjärt med antalet malicious klienter (1 attackerare: \textasciitilde8\% degradering, 2 attackerare: 25-43\% degradering)
    \item Non-IID data var betydligt mer känsligt för attacken (42.7\% degradering) jämfört med IID data (24.8\% degradering) vid 2 attackerare
    \item ROC-AUC minskade från 0.969 till 0.869-0.893, vilket indikerar kraftigt försämrad klassseparation
    \item F1-score drabbades hårdare än accuracy, särskilt för Non-IID data (från 61.7\% till 38.2\%)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/attack_impact_comparison.png}
\caption{Attack påverkan på FedAvg: Jämförelse mellan IID och Non-IID data. Non-IID data visar betydligt större sårbarhet (42.4\% accuracy med 2 attackerare) jämfört med IID data (56.4\%).}
\label{fig:attack_impact}
\end{figure}


\subsection{Del 2.2: FedProx med attacker}

\subsubsection{Beskrivning av FedProx}

FedProx är en variant av FedAvg som hanterar systemheterogenitet bättre genom att lägga till en proximal term i optimeringsmålet. Algoritmen lägger till en regulariseringterm som håller lokala modellvikter nära de globala viktarna:

\[\min_{w} F(w) + \frac{\mu}{2} \|w - w^t\|^2\]

där $w^t$ är de globala viktarna från föregående runda och $\mu$ är en hyperparameter (vi använde $\mu = 0.1$).

\textbf{Fördelar med FedProx}:
\begin{itemize}
    \item Stabilare konvergens vid heterogen data (Non-IID)
    \item Mindre känslig för outliers i klientuppdateringar
    \item Bättre hantering av partial participation (när inte alla klienter deltar)
\end{itemize}

\subsubsection{Experimentella resultat}

Vi upprepade samma 6 experiment som i Del 2.1 men med FedProx istället för FedAvg:

\textbf{IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = 74.5\%, F1 = 74.1\%, Kappa = 0.716, ROC-AUC = 0.966
    \item \textbf{1 attackerare}: Accuracy = 67.3\%, F1 = 66.6\%, Kappa = 0.636, ROC-AUC = 0.947
    \item \textbf{2 attackerare}: Accuracy = 55.3\%, F1 = 53.7\%, Kappa = 0.503, ROC-AUC = 0.882
\end{itemize}

\textbf{Non-IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = 75.0\%, F1 = 60.9\%, Kappa = 0.695, ROC-AUC = 0.966
    \item \textbf{1 attackerare}: Accuracy = 71.7\%, F1 = 56.7\%, Kappa = 0.652, ROC-AUC = 0.937
    \item \textbf{2 attackerare}: Accuracy = 56.2\%, F1 = 42.3\%, Kappa = 0.473, ROC-AUC = 0.880
\end{itemize}

\textbf{Jämförelse FedAvg vs FedProx}:
\begin{itemize}
    \item FedProx visade liknande sårbarhet mot label flipping som FedAvg på IID data (båda \textasciitilde55\% med 2 attackerare)
    \item På Non-IID data presterade FedProx betydligt bättre under attack: 56.2\% accuracy vs 42.4\% för FedAvg (13.8 procentenheter skillnad)
    \item FedProx baseline var något högre på Non-IID (75.0\% vs 74.0\%), vilket tyder på bättre hantering av dataheterogenitet
    \item Proximal regularisering gav viss robusthet mot attacken i Non-IID scenariot men inte i IID-scenariot
    \item Båda algoritmerna behöver dock explicit försvarsmekanism för effektivt skydd
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{figures/fedavg_vs_fedprox.png}
\caption{FedAvg vs FedProx robusthet: På Non-IID data med 2 attackerare presterar FedProx betydligt bättre (56.2\% vs 42.4\%), vilket visar att proximal regularisering ger naturlig resistens mot label flipping-attacker.}
\label{fig:fedavg_vs_fedprox}
\end{figure}


\subsection{Del 2.3: Försvarsmekanism 1 - Loss-Based Anomaly Detection}

\subsubsection{Beskrivning av försvarsmetod}

Vi implementerade en \textbf{loss-based anomaly detection} försvarsmekanism som filtrerar bort klienter med misstänkt höga träningsförluster innan aggregering. Rationaliteten är att malicious klienter som tränar på felaktiga labels kommer att ha signifikant högre förlust än ärliga klienter.

\textbf{Algoritm}:
\begin{enumerate}
    \item Samla alla klients viktuppdateringar och deras träningsförluster
    \item Beräkna 75:e percentilen av alla förluster
    \item Filtrera bort klienter vars förlust överstiger tröskelvärdet
    \item Aggregera endast de återstående klienternas vikter med FedAvg
\end{enumerate}

Matematiskt:
\[\text{threshold} = P_{75}(\{L_1, L_2, \ldots, L_K\})\]
\[\text{Accepted} = \{k : L_k \leq \text{threshold}\}\]
\[w^{t+1} = \frac{1}{|\text{Accepted}|} \sum_{k \in \text{Accepted}} w_k^t\]

\subsubsection{Implementation}

Försvaret implementerades genom att utöka FedAvg-klassen:
\begin{verbatim}
class FedAvgDefense(FedAvg):
    def aggregate_fit(self, server_round, results, failures):
        # Extract losses from client results
        losses = [fit_res.metrics["loss"] for _, fit_res in results]
        
        # Calculate threshold (75th percentile)
        threshold = np.percentile(losses, 
                                  self.loss_threshold_percentile)
        
        # Filter clients
        filtered_results = [
            (client, res) for (client, res), loss in 
            zip(results, losses) if loss <= threshold
        ]
        
        # Aggregate remaining clients
        return super().aggregate_fit(server_round, 
                                     filtered_results, failures)
\end{verbatim}

\subsubsection{Experimentella resultat}

\textbf{IID Data med 1 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = 68.6\%
    \item \textbf{Med loss-filter}: Accuracy = 68.5\% (praktiskt taget oförändrat)
    \item Försvaret var ineffektivt på IID data med endast 1 attackerare
\end{itemize}

\textbf{IID Data med 2 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = 56.4\%
    \item \textbf{Med loss-filter}: Accuracy = 52.2\% (försämring med 4.2 procentenheter)
    \item Försvaret var kontraproduktivt och filtrade bort ärliga klienter (false positives)
\end{itemize}

\textbf{Non-IID Data med 1 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = 68.2\%
    \item \textbf{Med loss-filter}: Accuracy = 69.6\% (23.9\% återhämtning av förlorad prestanda)
    \item Försvaret identifierade och exkluderade attackeraren effektivt
\end{itemize}

\textbf{Non-IID Data med 2 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = 42.4\%
    \item \textbf{Med loss-filter}: Accuracy = 49.9\% (23.7\% återhämtning av förlorad prestanda)
    \item Försvaret återställde 7.5 procentenheter, vilket är betydande förbättring
\end{itemize}

\textbf{Analys}:
\begin{itemize}
    \item Loss-based filtering var \textbf{mycket effektiv på Non-IID data} (23-24\% återhämtning) men \textbf{ineffektiv på IID data}
    \item På IID data var loss-variationen för liten mellan ärliga och malicious klienter, vilket ledde till false positives
    \item På Non-IID data hade malicious klienter signifikant högre förluster, vilket gjorde detektion enklare
    \item Försvaret återställde upp till 7.5 procentenheter accuracy på Non-IID data med 2 attackerare
    \item Enkel att implementera och kräver ingen modellmodifiering
    \item 75:e percentil-tröskelvärdet fungerade bra för Non-IID men behöver justeras för IID-scenarion
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/defense_effectiveness.png}
\caption{Försvarsmekanism effektivitet: Loss-based filtering visar dramatisk skillnad mellan IID (vänster) och Non-IID (höger) data. På Non-IID data återställer försvaret 7.5 procentenheter accuracy (från 42.4\% till 49.9\%), medan det på IID data är kontraproduktivt.}
\label{fig:defense_effectiveness}
\end{figure}


\subsection{Del 2.4: Slutsatser och Rekommendationer}

\subsubsection{Sammanfattning av experimentella resultat}

Våra experiment visade följande huvudsakliga fynd:

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/final_accuracy_comparison.png}
\caption{Översikt av alla experiment: Final accuracy för alla kombinationer av algoritm, datadistribution och antal attackerare. Visar tydligt att Non-IID data är mer sårbart och att försvarsmekanismen endast fungerar effektivt på Non-IID data.}
\label{fig:final_accuracy}
\end{figure}

\begin{enumerate}
    \item \textbf{Label flipping-attackens påverkan}:
    \begin{itemize}
        \item Attacken hade betydande negativ påverkan på modellprestanda
        \item Non-IID data var mer sårbart (42.7\% accuracy-degradering vid 2 attackerare) än IID data (24.8\%)
        \item Påverkan ökade icke-linjärt med antalet attackerare
    \end{itemize}
    
    \item \textbf{FedProx vs FedAvg}:
    \begin{itemize}
        \item FedProx visade bättre robusthet på Non-IID data (56.2\% vs 42.4\% vid 2 attackerare)
        \item På IID data var skillnaden minimal (båda \textasciitilde55-56\%)
        \item Proximal regularisering gav viss naturlig resistens mot attacken i heterogena scenario
    \end{itemize}
    
    \item \textbf{Loss-based defense effektivitet}:
    \begin{itemize}
        \item \textbf{Mycket effektiv på Non-IID data}: 23-24\% återhämtning av förlorad prestanda
        \item \textbf{Ineffektiv på IID data}: Ingen förbättring eller till och med försämring
        \item Anledning: På Non-IID data har malicious klienter signifikant högre förluster, vilket gör detektion enklare
        \item På IID data överlappar loss-distributionen mellan ärliga och malicious klienter
    \end{itemize}
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/f1_score_analysis.png}
\caption{F1-Score analys: Vänster visar att Non-IID data har lägre F1-score än IID data under attack. Höger visar försvarsmekanism påverkan på F1-score för Non-IID data, där försvaret förbättrar klassbalansen.}
\label{fig:f1_analysis}
\end{figure}

\subsubsection{Rekommendationer för praktisk tillämpning}

\textbf{För produktionsmiljöer}:
\begin{enumerate}
    \item \textbf{Vid Non-IID data}:
    \begin{itemize}
        \item Använd FedProx som basalgoritm (bättre naturlig robusthet)
        \item Implementera loss-based filtering med 75:e percentil-tröskel
        \item Förvänta 20-25\% återhämtning av prestanda under attack
    \end{itemize}
    
    \item \textbf{Vid IID data}:
    \begin{itemize}
        \item FedAvg och FedProx ger liknande resultat
        \item Loss-based filtering är ej tillräckligt - alternativa metoder behövs
        \item Överväg coordinate-wise median aggregation eller gradient clipping
        \item Implementera kompletterande säkerhetsåtgärder (t.ex. multi-Krum, trimmed mean)
    \end{itemize}
    
    \item \textbf{Generella råd}:
    \begin{itemize}
        \item Kombinera flera försvarsmekanism för robusthet
        \item Monitera loss-distributioner kontinuerligt för anomaly detection
        \item Använd Byzantine-robust aggregering vid misstänkt aktivitet
        \item Överväg reputationssystem för klienter över tid
    \end{itemize}
\end{enumerate}

\subsubsection{Begränsningar och framtida arbete}

\textbf{Begränsningar i vår studie}:
\begin{itemize}
    \item Endast en försvarsmetod implementerad (loss-based filtering)
    \item Endast label flipping-attack testades (andra attacktyper finns)
    \item Relativt litet antal klienter (5) - skalbarhet ej testad
    \item 75:e percentil-tröskelvärdet kan behöva anpassas per scenario
\end{itemize}

\textbf{Framtida arbete}:
\begin{itemize}
    \item Implementera och jämföra fler försvarsmetoder (Krum, Trimmed Mean, Median)
    \item Testa mot sofistikerade attacker (model poisoning, backdoor attacks)
    \item Utvärdera med större antal klienter och varierande attack-ratio
    \item Utveckla adaptiva tröskelvärden för loss-based filtering
    \item Undersöka hybrid-metoder som kombinerar flera försvarsmekanismer
\end{itemize}


\section{Referenser}
\begin{thebibliography}{9}

\bibitem{bim}
Kurakin, A., Goodfellow, I., \& Bengio, S. (2016). 
\textit{Adversarial examples in the physical world}. 
arXiv preprint arXiv:1607.02533.

\bibitem{blur}
Pillow Documentation. 
\textit{ImageFilter.GaussianBlur}. 
\url{https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html}

\end{thebibliography}

\end{document}
