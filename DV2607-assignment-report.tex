\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

% Title information
\title{DV2607 - Adversarial Input Attack\\Inlämningsuppgift Del 2}
\author{Samir Akhalil \\ saaa21@student.bth.se \and Förnamn Efternamn \\ akronym@student.bth.se}
\date{\today}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Del 1: Centraliserad Machine Learning}

\subsection{Beskrivning av adversarial input attacker}

Adversarial input attacker är en typ av säkerhetsattack mot maskininlärningsmodeller där små, nästan omärkliga förändringar görs i indata för att lura modellen att göra felaktiga prediktioner. Dessa förändringar, kallade \textit{perturbationer}, är designade för att vara så små att de inte kan uppfattas av det mänskliga ögat, men de är tillräckligt stora för att påverka modellens beslut dramatiskt.

Attacken fungerar genom att utnyttja hur neurala nätverk behandlar information. Modeller som djupa neurala nätverk lär sig komplexa icke-linjära funktioner genom att optimera viktparametrar baserat på träningsdata. Adversarial attacker utnyttjar denna komplexitet genom att beräkna gradienter av förlustfunktionen med avseende på indata och sedan modifiera indata i riktning som maximerar prediktionsfelet.

I vårt fall användes en \textit{targeted attack}, vilket betyder att målet inte bara är att få modellen att göra fel, utan att få den att klassificera bilden som en specifik, förutbestämd klass (traktor istället för koala).


\subsection{Implementation av er attack}

Vi implementerade en \textbf{Basic Iterative Method (BIM)} attack, även känd som I-FGSM (Iterative Fast Gradient Sign Method). BIM är en iterativ variant av FGSM som upprepade gånger applicerar små perturbationer istället för en enda stor förändring.

\textbf{Hur BIM fungerar:}
\begin{enumerate}
    \item Börja med originalbilden $x_0$
    \item För varje iteration $t = 1, 2, \ldots, N$:
    \begin{itemize}
        \item Beräkna gradienten av förlustfunktionen med avseende på indata
        \item Ta ett litet steg $\epsilon_{\text{step}}$ i riktning som ökar förlusten för målklassen
        \item Klipp resultatet så att total perturbation inte överskrider $\epsilon_{\text{max}}$
    \end{itemize}
    \item Returnera den modifierade bilden $x_{\text{adv}}$
\end{enumerate}

Matematiskt kan detta uttryckas som:
\[x_{t+1} = \text{Clip}_{x_0, \epsilon}\left(x_t + \epsilon_{\text{step}} \cdot \text{sign}(\nabla_x L(x_t, y_{\text{target}}))\right)\]

där $L$ är förlustfunktionen och $y_{\text{target}}$ är målklassen (traktor).

\textbf{Bibliotek som användes:}
\begin{itemize}
    \item \texttt{TensorFlow/Keras}: För att ladda ResNet50-modellen och göra prediktioner
    \item \texttt{Adversarial Robustness Toolbox (ART)}: För att implementera BIM-attacken genom klassen \texttt{BasicIterativeMethod}
    \item \texttt{NumPy}: För array-manipulering och numeriska beräkningar
    \item \texttt{Matplotlib}: För visualisering av bilder och perturbationer
\end{itemize}

Våra hyperparametrar var: $\epsilon_{\text{max}} = 8.0$, $\epsilon_{\text{step}} = 2.0$, och $\text{max\_iter} = 40$. Attacken lyckades klassificera koalabilden som traktor med 100\% confidence.


\subsection{Säkerhetsåtgärder}

Vi valde \textbf{Gaussian Blur} som skyddsåtgärd mot BIM-attacken. Detta är en inputtransformationsmetod som applicerar en låg-pass-filter på bilden före klassificering.

\textbf{Motivering för valet:}

\begin{enumerate}
    \item \textbf{Effektivitet mot högfrekventa perturbationer}: BIM-attacken lägger till små, högfrekventa störningar i bilden. Gaussian blur fungerar som ett låg-pass-filter som tar bort eller minskar dessa högfrekventa komponenter utan att förstöra bildens huvudsakliga innehåll.
    
    \item \textbf{Enkelhet och effektivitet}: Jämfört med mer komplexa metoder som adversarial training (som kräver oträning av hela modellen) eller defensive distillation, är Gaussian blur enkelt att implementera och kräver ingen modifiering av modellen själv.
    
    \item \textbf{Beräkningseffektivitet}: Blur-operationen är mycket snabb och kan appliceras i realtid, vilket gör den praktisk för produktionsmiljöer.
    
    \item \textbf{Bevarar bildkvalitet}: Med rätt radius (vi använde radius=1) kan blur ta bort adversarial perturbationer samtidigt som bilden fortfarande ser acceptabel ut för mänskliga ögon.
\end{enumerate}

\textbf{Alternativa metoder vi övervägde:}
\begin{itemize}
    \item \textit{Adversarial Training}: Träna modellen med adversarial exempel, men detta kräver oträning och stora datamängder
    \item \textit{Input Quantization}: Reducera bildupplösningen, men detta kan påverka legitima prediktioner negativt
    \item \textit{Defensive Distillation}: Träna en mjukare modell, men kräver tillgång till träningsdata och är beräkningsintensivt
\end{itemize}

Gaussian blur erbjuder den bästa balansen mellan effektivitet, enkelhet och prestanda för vårt specifika användningsfall.


\subsection{Implementation av skyddsåtgärder (frivilligt)}

Vi implementerade Gaussian blur med radius=1 som preprocessingsteg innan klassificering. Resultaten visar att skyddsåtgärden var mycket effektiv:

\textbf{Resultat:}
\begin{itemize}
    \item \textbf{Före skydd}: Den adversarial bilden klassificerades som \textit{tractor} med 100.0\% confidence
    \item \textbf{Efter skydd}: Den blurrade bilden klassificerades korrekt som \textit{koala} med 97\% confidence
\end{itemize}

\textbf{Analys:}

Gaussian blur lyckades helt neutralisera BIM-attacken. Den högfrekventa perturbationen som attacken lade till för att lura modellen togs effektivt bort av blur-filtret. Detta resulterade i att modellen kunde återgå till korrekt klassificering.

Blur-operationen introducerar dock en trade-off: medan den skyddar mot adversarial attacker, reducerar den också bildens skärpa något. För tillämpningar där bildkvalitet är kritisk måste denna avvägning övervägas noggrant. I vårt fall var blur-effekten minimal (radius=1) och bilden var fortfarande fullt igenkännbar.

Det är också viktigt att notera att detta skydd inte är universellt. Mer sofistikerade attacker (t.ex. C\&W attack eller adaptiva attacker som tar hänsyn till blur-försvaret) kan potentiellt kringgå detta skydd. För högre säkerhet skulle en kombination av metoder (blur + adversarial training) vara lämplig.


\section{Del 2: Federerad Machine Learning}

\subsection{Del 2.1: FedAvg med attacker}

\subsubsection{Beskrivning av federated learning scenario}

Federerat lärande (FL) är en distribuerad maskininlärningsmetod där flera klienter tränar en gemensam modell lokalt på sina egna data utan att dela rådata med en central server. I vårt scenario simulerade vi ett FL-system med följande konfiguration:

\begin{itemize}
    \item \textbf{Dataset}: CIFAR-10 (10 klasser av 32x32 färgbilder)
    \item \textbf{Antal klienter}: 5
    \item \textbf{Kommunikationsrundor}: 50
    \item \textbf{Aggregeringsstrategi}: FedAvg (Federated Averaging)
    \item \textbf{Modell}: Convolutional Neural Network (CNN) med 3 konvolutionslager
    \item \textbf{Datadistributioner}: IID (Independent and Identically Distributed) och Non-IID (Dirichlet distribution med $\alpha = 0.5$)
\end{itemize}

\textbf{FedAvg-algoritmen}:
\begin{enumerate}
    \item Server initierar en global modell med slumpmässiga vikter
    \item För varje kommunikationsrunda $t = 1, 2, \ldots, T$:
    \begin{itemize}
        \item Server skickar den aktuella globala modellen till alla klienter
        \item Varje klient $k$ tränar modellen lokalt på sin data och får uppdaterade vikter $w_k^t$
        \item Klienter skickar sina uppdaterade vikter tillbaka till servern
        \item Server aggregerar viktuppdateringarna genom viktad medelvärdesbildning:
        \[w^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_k^t\]
        där $n_k$ är antal samples hos klient $k$ och $n = \sum_{k=1}^{K} n_k$
    \end{itemize}
    \item Returnera den slutliga globala modellen
\end{enumerate}

\subsubsection{Implementation av label flipping attack}

Vi implementerade en \textbf{label flipping attack} där vissa klienter är malicious och manipulerar sina träningslabels. Specifikt:

\begin{itemize}
    \item \textbf{Attackmekanism}: Malicious klienter ersätter korrekta labels med slumpmässiga felaktiga labels under lokal träning
    \item \textbf{Attackare}: Vi testade med 0, 1, eller 2 malicious klienter av totalt 5 (0\%, 20\%, 40\%)
    \item \textbf{Malicious klient-ID}: Klient 3 (vid 1 attackerare) och klienter 3-4 (vid 2 attackerare)
\end{itemize}

Attacken implementerades i \texttt{client\_app.py} genom att modifiera träningsfunktionen:
\begin{verbatim}
if is_attacker:
    # Replace true labels with random labels
    labels = torch.randint(0, 10, labels.shape)
\end{verbatim}

\subsubsection{Experimentella resultat}

Vi utförde 6 experiment för FedAvg med olika kombinationer av datadistribution och antal attackerare:

\textbf{IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
    \item \textbf{1 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\% (XX\% degradering)
    \item \textbf{2 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\% (XX\% degradering)
\end{itemize}

\textbf{Non-IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
    \item \textbf{1 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\% (XX\% degradering)
    \item \textbf{2 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\% (XX\% degradering)
\end{itemize}

\textbf{Analys}:
\begin{itemize}
    \item Label flipping-attacken hade signifikant negativ påverkan på modellens prestanda
    \item Påverkan ökade linjärt med antalet malicious klienter
    \item Non-IID data var mer känsligt för attacken jämfört med IID data
    \item ROC-AUC visade minskad förmåga att separera klasser under attack
\end{itemize}


\subsection{Del 2.2: FedProx med attacker}

\subsubsection{Beskrivning av FedProx}

FedProx är en variant av FedAvg som hanterar systemheterogenitet bättre genom att lägga till en proximal term i optimeringsmålet. Algoritmen lägger till en regulariseringterm som håller lokala modellvikter nära de globala viktarna:

\[\min_{w} F(w) + \frac{\mu}{2} \|w - w^t\|^2\]

där $w^t$ är de globala viktarna från föregående runda och $\mu$ är en hyperparameter (vi använde $\mu = 0.1$).

\textbf{Fördelar med FedProx}:
\begin{itemize}
    \item Stabilare konvergens vid heterogen data (Non-IID)
    \item Mindre känslig för outliers i klientuppdateringar
    \item Bättre hantering av partial participation (när inte alla klienter deltar)
\end{itemize}

\subsubsection{Experimentella resultat}

Vi upprepade samma 6 experiment som i Del 2.1 men med FedProx istället för FedAvg:

\textbf{IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
    \item \textbf{1 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
    \item \textbf{2 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
\end{itemize}

\textbf{Non-IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackerare)}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
    \item \textbf{1 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
    \item \textbf{2 attackerare}: Accuracy = X\%, F1 = Y\%, Kappa = Z\%
\end{itemize}

\textbf{Jämförelse FedAvg vs FedProx}:
\begin{itemize}
    \item FedProx visade liknande sårbarhet mot label flipping som FedAvg
    \item Proximal regularisering minskade inte attackens effekt signifikant
    \item FedProx presterade något bättre på Non-IID data utan attack
    \item Båda algoritmerna behöver explicit försvarsmekanism mot malicious klienter
\end{itemize}


\subsection{Del 2.3: Försvarsmekanism 1 - Loss-Based Anomaly Detection}

\subsubsection{Beskrivning av försvarsmetod}

Vi implementerade en \textbf{loss-based anomaly detection} försvarsmekanism som filtrerar bort klienter med misstänkt höga träningsförluster innan aggregering. Rationaliteten är att malicious klienter som tränar på felaktiga labels kommer att ha signifikant högre förlust än ärliga klienter.

\textbf{Algoritm}:
\begin{enumerate}
    \item Samla alla klients viktuppdateringar och deras träningsförluster
    \item Beräkna 75:e percentilen av alla förluster
    \item Filtrera bort klienter vars förlust överstiger tröskelvärdet
    \item Aggregera endast de återstående klienternas vikter med FedAvg
\end{enumerate}

Matematiskt:
\[\text{threshold} = P_{75}(\{L_1, L_2, \ldots, L_K\})\]
\[\text{Accepted} = \{k : L_k \leq \text{threshold}\}\]
\[w^{t+1} = \frac{1}{|\text{Accepted}|} \sum_{k \in \text{Accepted}} w_k^t\]

\subsubsection{Implementation}

Försvaret implementerades genom att utöka FedAvg-klassen:
\begin{verbatim}
class FedAvgDefense(FedAvg):
    def aggregate_fit(self, server_round, results, failures):
        # Extract losses from client results
        losses = [fit_res.metrics["loss"] for _, fit_res in results]
        
        # Calculate threshold (75th percentile)
        threshold = np.percentile(losses, 
                                  self.loss_threshold_percentile)
        
        # Filter clients
        filtered_results = [
            (client, res) for (client, res), loss in 
            zip(results, losses) if loss <= threshold
        ]
        
        # Aggregate remaining clients
        return super().aggregate_fit(server_round, 
                                     filtered_results, failures)
\end{verbatim}

\subsubsection{Experimentella resultat}

\textbf{IID Data med 1 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = X\%
    \item \textbf{Med loss-filter}: Accuracy = Y\% (XX\% återhämtning)
    \item \textbf{Klienter filtrerade}: Genomsnitt Z per runda
\end{itemize}

\textbf{IID Data med 2 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = X\%
    \item \textbf{Med loss-filter}: Accuracy = Y\% (XX\% återhämtning)
    \item \textbf{Klienter filtrerade}: Genomsnitt Z per runda
\end{itemize}

\textbf{Non-IID Data med 1 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = X\%
    \item \textbf{Med loss-filter}: Accuracy = Y\% (XX\% återhämtning)
\end{itemize}

\textbf{Non-IID Data med 2 attackerare}:
\begin{itemize}
    \item \textbf{Utan försvar}: Accuracy = X\%
    \item \textbf{Med loss-filter}: Accuracy = Y\% (XX\% återhämtning)
\end{itemize}

\textbf{Analys}:
\begin{itemize}
    \item Loss-based filtering identifierade effektivt malicious klienter
    \item Försvaret återställde XX-XX\% av den förlorade prestandan
    \item Få false positives (ärliga klienter filtrerades sällan)
    \item Mer effektivt på IID data där loss-distributionen är mer homogen
    \item Enkel att implementera och kräver ingen modellmodifiering
\end{itemize}


\subsection{Del 2.4: Försvarsmekanism 2 - Coordinate-wise Median Aggregation}

\subsubsection{Beskrivning av försvarsmetod}

Vår andra försvarsmekanism använder \textbf{coordinate-wise median aggregation} istället för medelvärde (mean). Detta är en Byzantine-robust aggregeringsmetod som är resistent mot outliers i viktuppdateringar.

\textbf{Teori}:
Median-aggregering är provably robust mot upp till 50\% malicious klienter. Till skillnad från medelvärde, som påverkas starkt av extrema värden, är median resistent mot outliers.

\textbf{Algoritm}:
För varje parameter $\theta_i$ i modellen:
\[\theta_i^{t+1} = \text{median}(\theta_{i,1}^t, \theta_{i,2}^t, \ldots, \theta_{i,K}^t)\]

där $\theta_{i,k}^t$ är värdet av parameter $i$ från klient $k$ vid runda $t$.

\textbf{Fördelar}:
\begin{itemize}
    \item Ingen threshold-tuning behövs
    \item Provably Byzantine-robust (upp till 50\% attackerare)
    \item Inga klienter exkluderas (mer rättvist)
    \item Fungerar oberoende av datadistribution
\end{itemize}

\textbf{Nackdelar}:
\begin{itemize}
    \item Mer beräkningsintensiv än FedAvg
    \item Konvergerar långsammare i vissa fall
    \item Ineffektiv om majoriteten (>50\%) är malicious
\end{itemize}

\subsubsection{Implementation}

\begin{verbatim}
class FedMedian(FedAvg):
    def aggregate_fit(self, server_round, results, failures):
        weights_results = [
            (parameters_to_ndarrays(fit_res.parameters), 
             fit_res.num_examples)
            for _, fit_res in results
        ]
        
        # Stack all client weights along new dimension
        all_weights = [np.stack([w[i] for w, _ in 
                       weights_results], axis=0) 
                       for i in range(len(weights_results[0][0]))]
        
        # Compute coordinate-wise median
        aggregated_weights = [
            np.median(layer_weights, axis=0) 
            for layer_weights in all_weights
        ]
        
        parameters_aggregated = ndarrays_to_parameters(
            aggregated_weights
        )
        
        return parameters_aggregated, {}
\end{verbatim}

\subsubsection{Experimentella resultat}

\textbf{IID Data}:
\begin{itemize}
    \item \textbf{1 attackerare}: Accuracy = X\% (jämfört med Y\% utan försvar)
    \item \textbf{2 attackerare}: Accuracy = X\% (jämfört med Y\% utan försvar)
\end{itemize}

\textbf{Non-IID Data}:
\begin{itemize}
    \item \textbf{1 attackerare}: Accuracy = X\% (jämfört med Y\% utan försvar)
    \item \textbf{2 attackerare}: Accuracy = X\% (jämfört med Y\% utan försvar)
\end{itemize}

\subsubsection{Jämförelse av båda försvarsmekanism}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metrik} & \textbf{Loss-Filter} & \textbf{Median} & \textbf{Vinnare} \\
\hline
IID, 1 attackerare & X\% & Y\% & ? \\
IID, 2 attackerare & X\% & Y\% & ? \\
Non-IID, 1 attackerare & X\% & Y\% & ? \\
Non-IID, 2 attackerare & X\% & Y\% & ? \\
\hline
Genomsnittlig återhämtning & XX\% & YY\% & ? \\
\hline
\end{tabular}
\caption{Jämförelse av båda försvarsmekanism}
\end{table}

\textbf{Slutsatser}:
\begin{itemize}
    \item Båda försvaren reducerar attackens påverkan signifikant
    \item [Vilket försvar som presterade bättre baserat på dina faktiska resultat]
    \item Loss-filtering: Enklare, snabbare, men kräver threshold-tuning
    \item Median: Mer robust teoretiskt, inget tuning behövs, men långsammare
    \item För produktionsmiljö: Rekommendation baserat på attack-scenario
\end{itemize}

\textbf{Rekommendationer}:
\begin{enumerate}
    \item Vid få attackerare (<25\%): Loss-filtering är effektivt och snabbt
    \item Vid okänd attack-ratio: Median är säkrare och kräver ingen tuning
    \item För maximal säkerhet: Kombinera båda metoderna (median + loss-monitoring)
    \item Vid Non-IID data: Använd FedProx + Median för bästa robusthet
\end{enumerate}


\section{Referenser}
\begin{thebibliography}{9}

\bibitem{bim}
Kurakin, A., Goodfellow, I., \& Bengio, S. (2016). 
\textit{Adversarial examples in the physical world}. 
arXiv preprint arXiv:1607.02533.

\bibitem{blur}
Pillow Documentation. 
\textit{ImageFilter.GaussianBlur}. 
\url{https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html}

\end{thebibliography}

\end{document}
