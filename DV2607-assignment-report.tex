\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[swedish,english]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\hypersetup{
    colorlinks=false,
    hidelinks
}

% Title information
\title{DV2607 - Adversarial Input Attack\\Inlämningsuppgift Del 2}
\author{Samir Akhalil \\ saaa21@student.bth.se \and Mhd Malek Kisaniehj \\ mhki22@student.bth.se}
\date{\today}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Del 1: Centraliserad Machine Learning}

\subsection{Beskrivning av adversarial input attacker}

Adversarial input attacker är när man gör små förändringar i indata för att lura en maskininlärningsmodell. Förändringarna (perturbationer) är så små att människor inte märker dem, men de får modellen att göra helt fel klassificeringar.

Attacken fungerar genom att man beräknar gradienten av modellens förlustfunktion och ändrar bilden i den riktningen som maximerar felet. Neurala nätverk är känsliga för detta eftersom de lär sig komplexa funktioner som kan manipuleras.

Vi använde en \textit{targeted attack}, vilket betyder att vi ville få modellen att klassificera koalabilden som just traktor, inte bara vilken fel klass som helst.


\subsection{Implementation av er attack}

Vi använde \textbf{Basic Iterative Method (BIM)}, också kallad I-FGSM. BIM applicerar små perturbationer upprepade gånger istället för en stor förändring på en gång.

\textbf{Hur det fungerar:}
\begin{enumerate}
\item Börja med originalbilden $x_0$
\item För varje iteration ($t = 1, 2, \ldots, N$):
\begin{itemize}
\item Beräkna gradienten av förlusten
\item Ta ett litet steg $\epsilon_{\text{step}}$ mot målklassen
\item Se till att total förändring inte blir större än $\epsilon_{\text{max}}$
\end{itemize}
\item Returnera den adversariella bilden
\end{enumerate}

Formeln blir:
\[x_{t+1} = \text{Clip}_{x_0, \epsilon}\left(x_t - \epsilon_{\text{step}} \cdot \text{sign}(\nabla_x L(x_t, y_{\text{target}}))\right)\]

där $L$ är förlustfunktionen och $y_{\text{target}}$ är målklassen (traktor). Minustecknet används eftersom vi vill minimera förlusten för just målklassen.

\textbf{Bibliotek som användes:}
\begin{itemize}
    \item \texttt{TensorFlow/Keras}: För att ladda ResNet50-modellen och göra prediktioner
    \item \texttt{Adversarial Robustness Toolbox (ART)}: För att implementera BIM-attacken genom klassen \texttt{BasicIterativeMethod}
    \item \texttt{NumPy}: För array-manipulering och numeriska beräkningar
    \item \texttt{Matplotlib}: För visualisering av bilder och perturbationer
\end{itemize}

Våra hyperparametrar var: $\epsilon_{\text{max}} = 8.0$, $\epsilon_{\text{step}} = 2.0$, och $\text{max\_iter} = 50$. 

\textbf{Experimentella resultat:}
\begin{itemize}
    \item \textbf{Original klassificering}: Koala (ImageNet klass 105) med 99.8\% confidence
    \item \textbf{Adversarial klassificering}: Traktor (ImageNet klass 866) med 100\% confidence
    \item \textbf{Perturbation metrics}: $L_\infty$ norm = 8.0, $L_2$ norm $\approx$ 1990
    \item \textbf{Visuell imperceptibilitet}: Maximal pixelförändring endast 3.1\% av totalt värde (8/255)
\end{itemize}

Attacken lyckades helt - modellen klassificerade koalabilden som traktor med 99.9\% confidence.


\subsection{Säkerhetsåtgärder}

Vi valde \textbf{Gaussian Blur} som skyddsåtgärd mot BIM-attacken. Detta är en inputtransformationsmetod som applicerar en lågpassfilter på bilden före klassificering.

\textbf{Varför vi valde Gaussian blur:}

\begin{itemize}
    \item BIM-attacken lägger till små högfrekventa störningar. Gaussian blur fungerar som ett låg-pass-filter som tar bort dessa utan att förstöra bildens huvudsakliga innehåll.
    
    \item Det är enkelt att implementera jämfört med adversarial training eller defensive distillation som kräver omträning av modellen.
    
    \item Blur är snabbt och kan köras i realtid.
    
    \item Med rätt radius (vi använde 1) bevaras bildkvaliteten tillräckligt för att fortfarande se vad bilden föreställer.
\end{itemize}

\textbf{Andra metoder vi tittade på:}
\begin{itemize}
    \item \textit{Adversarial Training}: Träna med adversarial exempel, men det tar lång tid och kräver mycket data
    \item \textit{Input Quantization}: Sänk upplösningen, men det påverkar även vanliga bilder negativt
    \item \textit{Defensive Distillation}: Träna en mjukare modell, men det är beräkningsintensivt
\end{itemize}

Gaussian blur var den enklaste metoden som ändå fungerade bra.


\subsection{Implementation av skyddsåtgärder (frivilligt)}

Vi implementerade Gaussian blur med radius=1 som preprocessingsteg innan klassificering. Resultaten visar att skyddsåtgärden var mycket effektiv:

\textbf{Resultat:}
\begin{itemize}
    \item \textbf{Före skydd}: den adversariella bilden klassificerades som \textit{tractor} (klass 866) med 99.9\% confidence
    \item \textbf{Efter skydd}: Den blurrade bilden klassificerades korrekt som \textit{koala} (klass 105) med 99\% confidence
\end{itemize}

\textbf{Analys:}

Gaussian blur funkade riktigt bra - den tog bort de högfrekventa perturbationerna som attacken lade till, så modellen klassificerade bilden korrekt igen.

Nackdelen är att blur gör bilden lite suddig. Men med radius=1 var effekten så liten att bilden fortfarande var tydlig.

Skyddet är inte perfekt. Mer avancerade attacker (t.ex. C\&W attack) kan kringgå blur genom att ta hänsyn till det i attacken. För bättre säkerhet skulle man behöva kombinera flera metoder.


\section{Part 2: Federated learning scenario}

\subsection{Part 2.1: FedAvg with Attacks}

\subsubsection{Description of Federated Learning Scenario}

In Federated Learning (FL), multiple clients train a shared model locally on their own data without sharing raw data with a central server. We simulated an FL system with:

\begin{itemize}
    \item \textbf{Dataset}: CIFAR-10 (10 classes of 32x32 color images)
    \item \textbf{Number of clients}: 5
    \item \textbf{Communication rounds}: 50
    \item \textbf{Aggregation strategy}: FedAvg (Federated Averaging)
    \item \textbf{Model}: Convolutional Neural Network (CNN) with 3 convolutional layers
    \item \textbf{Data distributions}: IID (Independent and Identically Distributed) and Non-IID (Dirichlet distribution with $\alpha = 0.5$)
\end{itemize}

\textbf{FedAvg Algorithm}:
\begin{enumerate}
    \item Server initializes a global model
    \item For each round ($t = 1, 2, \ldots, T$):
    \begin{itemize}
        \item Server sends model to all clients
        \item Each client $k$ trains locally and gets new weights $w_k^t$
        \item Clients send weights back to server
        \item Server averages the weights:
        \[w^{t+1} = \sum_{k=1}^{K} \frac{n_k}{n} w_k^t\]
        where $n_k$ is the number of samples at client $k$
    \end{itemize}
    \item Return final model
\end{enumerate}

\subsubsection{Implementation of Label Flipping Attack}

We implemented a \textbf{label flipping attack} where malicious clients manipulate their training labels:

\begin{itemize}
    \item Malicious clients replace correct labels with random incorrect labels during training
    \item We tested with 0, 1, or 2 attackers out of 5 clients (0\%, 20\%, 40\%)
    \item Attacker IDs: Client 3 (1 attacker) or clients 3-4 (2 attackers)
\end{itemize}

The attack was implemented in task.py within the training function:

\begin{lstlisting}
if is_attacker:
    if attack_type == "label_flipping":
        labels = torch.randint(0, 10, labels.shape, device=device)
\end{lstlisting}

\subsubsection{Experimental Results}

We conducted 6 experiments for FedAvg with different combinations of data distribution and number of attackers:

\textbf{IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackers)}: Accuracy = 75.0\%, F1 = 74.3\%, Kappa = 0.722, ROC-AUC = 0.969
    \item \textbf{1 attacker}: Accuracy = 68.6\%, F1 = 67.5\%, Kappa = 0.650, ROC-AUC = 0.948 (8.5\% degradation)
    \item \textbf{2 attackers}: Accuracy = 56.4\%, F1 = 55.5\%, Kappa = 0.515, ROC-AUC = 0.893 (24.8\% degradation)
\end{itemize}

\textbf{Non-IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackers)}: Accuracy = 74.0\%, F1 = 61.7\%, Kappa = 0.686, ROC-AUC = 0.968
    \item \textbf{1 attacker}: Accuracy = 68.2\%, F1 = 52.3\%, Kappa = 0.612, ROC-AUC = 0.933 (7.9\% degradation)
    \item \textbf{2 attackers}: Accuracy = 42.4\%, F1 = 38.2\%, Kappa = 0.353, ROC-AUC = 0.869 (42.7\% degradation)
\end{itemize}

\textbf{Analysis}:
\begin{itemize}
    \item The attack had a big negative impact on performance
    \item Impact increased non-linearly (1 attacker: \textasciitilde8\% drop, 2 attackers: 25-43\% drop)
    \item Non-IID data was more vulnerable (42.7\% degradation vs 24.8\% for IID with 2 attackers)
    \item ROC-AUC dropped from 0.969 to 0.869-0.893, showing degraded class separation
    \item F1-score dropped more than accuracy, especially for Non-IID (from 61.7\% to 38.2\%)
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/attack_impact_comparison.png}
\caption{Attack impact on FedAvg: Comparison between IID and Non-IID data. Non-IID data shows significantly greater vulnerability (42.4\% accuracy with 2 attackers) compared to IID data (56.4\%).}
\label{fig:attack_impact}
\end{figure}


\subsection{Part 2.2: FedProx with Attacks}

\subsubsection{Description of FedProx}

FedProx is a variant of FedAvg that handles heterogeneous data better by adding a proximal term. It adds a regularization term that keeps local weights close to the global weights:

\[\min_{w} F(w) + \frac{\mu}{2} \|w - w^t\|^2\]

where $w^t$ are the global weights from the previous round and $\mu$ is a hyperparameter (we used $\mu = 0.1$).

\textbf{Why use FedProx}:
\begin{itemize}
    \item More stable convergence with Non-IID data
    \item Less sensitive to outliers
    \item Handles partial client participation better
\end{itemize}

\subsubsection{Experimental Results}

We repeated the same 6 experiments as in Part 2.1 but with FedProx instead of FedAvg:

\textbf{IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackers)}: Accuracy = 74.5\%, F1 = 74.1\%, Kappa = 0.716, ROC-AUC = 0.966
    \item \textbf{1 attacker}: Accuracy = 67.3\%, F1 = 66.6\%, Kappa = 0.636, ROC-AUC = 0.947
    \item \textbf{2 attackers}: Accuracy = 55.3\%, F1 = 53.7\%, Kappa = 0.503, ROC-AUC = 0.882
\end{itemize}

\textbf{Non-IID Data Distribution:}
\begin{itemize}
    \item \textbf{Baseline (0 attackers)}: Accuracy = 75.0\%, F1 = 60.9\%, Kappa = 0.695, ROC-AUC = 0.966
    \item \textbf{1 attacker}: Accuracy = 71.7\%, F1 = 56.7\%, Kappa = 0.652, ROC-AUC = 0.937
    \item \textbf{2 attackers}: Accuracy = 56.2\%, F1 = 42.3\%, Kappa = 0.473, ROC-AUC = 0.880
\end{itemize}

\textbf{Comparison FedAvg vs FedProx}:
\begin{itemize}
    \item FedProx showed similar vulnerability to label flipping as FedAvg on IID data (both \textasciitilde55\% with 2 attackers)
    \item On Non-IID data, FedProx performed significantly better under attack: 56.2\% accuracy vs 42.4\% for FedAvg (13.8 percentage points difference)
    \item FedProx baseline was slightly higher on Non-IID (75.0\% vs 74.0\%), suggesting better handling of data heterogeneity
    \item Proximal regularization provided some robustness against the attack in the Non-IID scenario but not in the IID scenario
    \item Both algorithms require explicit defense mechanisms for effective protection
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{figures/fedavg_vs_fedprox.png}
\caption{FedAvg vs FedProx robustness: On Non-IID data with 2 attackers, FedProx performs significantly better (56.2\% vs 42.4\%), showing that proximal regularization provides natural resistance against label flipping attacks.}
\label{fig:fedavg_vs_fedprox}
\end{figure}


\subsection{Part 2.3: Defense Mechanism 1 - Loss-Based Anomaly Detection}

\subsubsection{Description of Defense Method}

We implemented a \textbf{loss-based filtering} defense that filters out clients with high training losses before aggregation. The idea is that malicious clients training on incorrect labels will have much higher loss than honest clients.

\textbf{Algorithm}:
\begin{enumerate}
    \item Collect all client weights and their losses
    \item Calculate 75th percentile of losses as threshold
    \item Filter out clients with loss above threshold
    \item Aggregate remaining clients with FedAvg
\end{enumerate}

Mathematically:
\[\text{threshold} = P_{75}(\{L_1, L_2, \ldots, L_K\})\]
\[\text{Accepted} = \{k : L_k \leq \text{threshold}\}\]
\[w^{t+1} = \frac{1}{|\text{Accepted}|} \sum_{k \in \text{Accepted}} w_k^t\]

\subsubsection{Implementation}

The defense was implemented by extending the FedAvg class:
\begin{verbatim}
class FedAvgDefense(FedAvg):
    def aggregate_fit(self, server_round, results, failures):
        # Extract losses from client results
        losses = [fit_res.metrics["loss"] for _, fit_res in results]
        
        # Calculate threshold (75th percentile)
        threshold = np.percentile(losses, 
                                  self.loss_threshold_percentile)
        
        # Filter clients
        filtered_results = [
            (client, res) for (client, res), loss in 
            zip(results, losses) if loss <= threshold
        ]
        
        # Aggregate remaining clients
        return super().aggregate_fit(server_round, 
                                     filtered_results, failures)
\end{verbatim}

\subsubsection{Experimental Results}

\textbf{IID Data with 1 attacker}:
\begin{itemize}
    \item \textbf{Without defense}: Accuracy = 68.6\%
    \item \textbf{With loss-filter}: Accuracy = 68.5\% (practically unchanged)
    \item The defense was ineffective on IID data with only 1 attacker
\end{itemize}

\textbf{IID Data with 2 attackers}:
\begin{itemize}
    \item \textbf{Without defense}: Accuracy = 56.4\%
    \item \textbf{With loss-filter}: Accuracy = 52.2\% (degradation of 4.2 percentage points)
    \item The defense was counterproductive and filtered out honest clients (false positives)
\end{itemize}

\textbf{Non-IID Data with 1 attacker}:
\begin{itemize}
    \item \textbf{Without defense}: Accuracy = 68.2\%
    \item \textbf{With loss-filter}: Accuracy = 69.6\% (23.9\% recovery of lost performance)
    \item The defense identified and excluded the attacker effectively
\end{itemize}

\textbf{Non-IID Data with 2 attackers}:
\begin{itemize}
    \item \textbf{Without defense}: Accuracy = 42.4\%
    \item \textbf{With loss-filter}: Accuracy = 49.9\% (23.7\% recovery of lost performance)
    \item The defense recovered 7.5 percentage points, a significant improvement
\end{itemize}

\textbf{Analysis}:
\begin{itemize}
    \item Loss-based filtering worked well on Non-IID data (23-24\% recovery) but not on IID data
    \item On IID data, honest and malicious clients had similar losses, causing false positives
    \item On Non-IID data, malicious clients had much higher losses, making them easy to detect
    \item The defense recovered 7.5 percentage points on Non-IID with 2 attackers
    \item Simple to implement, no model changes needed
    \item The 75th percentile threshold worked for Non-IID but not IID
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/defense_effectiveness.png}
\caption{Defense mechanism effectiveness: Loss-based filtering shows dramatic difference between IID (left) and Non-IID (right) data. On Non-IID data, the defense recovers 7.5 percentage points accuracy (from 42.4\% to 49.9\%), while on IID data it is counterproductive.}
\label{fig:defense_effectiveness}
\end{figure}


\subsection{Part 2.4: Conclusions and Recommendations}

\subsubsection{Summary of Experimental Results}

Our experiments revealed the following main findings:

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/final_accuracy_comparison.png}
\caption{Overview of all experiments: Final accuracy for all combinations of algorithm, data distribution, and number of attackers. Clearly shows that Non-IID data is more vulnerable and that the defense mechanism only works effectively on Non-IID data.}
\label{fig:final_accuracy}
\end{figure}

\begin{enumerate}
    \item \textbf{Label flipping attack impact}:
    \begin{itemize}
        \item The attack had significant negative impact on model performance
        \item Non-IID data was more vulnerable (42.7\% accuracy degradation with 2 attackers) than IID data (24.8\%)
        \item Impact increased non-linearly with the number of attackers
    \end{itemize}
    
    \item \textbf{FedProx vs FedAvg}:
    \begin{itemize}
        \item FedProx showed better robustness on Non-IID data (56.2\% vs 42.4\% with 2 attackers)
        \item On IID data, the difference was minimal (both \textasciitilde55-56\%)
        \item Proximal regularization provided some natural resistance against the attack in heterogeneous scenarios
    \end{itemize}
    
    \item \textbf{Loss-based defense effectiveness}:
    \begin{itemize}
        \item \textbf{Highly effective on Non-IID data}: 23-24\% recovery of lost performance
        \item \textbf{Ineffective on IID data}: No improvement or even degradation
        \item Reason: On Non-IID data, malicious clients have significantly higher losses, making detection easier
        \item On IID data, the loss distribution overlaps between honest and malicious clients
    \end{itemize}
\end{enumerate}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/f1_score_analysis.png}
\caption{F1-Score analysis: Left shows that Non-IID data has lower F1-score than IID data under attack. Right shows defense mechanism impact on F1-score for Non-IID data, where the defense improves class balance.}
\label{fig:f1_analysis}
\end{figure}

\subsubsection{Limitations and Future Work}

\textbf{Limitations of our study}:
\begin{itemize}
    \item Only label flipping attack tested (other attack types exist)
    \item Only one defense method implemented (loss-based filtering)
    \item The 75th percentile threshold may need adjustment per scenario
\end{itemize}

\textbf{Future work}:
\begin{itemize}
    \item Implement and compare more defense methods (Krum, Trimmed Mean, Median)
    \item Test against sophisticated attacks (model poisoning, backdoor attacks)
    \item Evaluate with larger number of clients and varying attack ratios
    \item Develop adaptive thresholds for loss-based filtering
    \item Investigate hybrid methods that combine multiple defense mechanisms
\end{itemize}


\section{Referenser}
\begin{thebibliography}{9}

\bibitem{flower}
Beutel, D. J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li, K. H., Parcollet, T., de Gusmão, P. P. B., \& Lane, N. D. (2020). 
\textit{Flower: A Friendly Federated Learning Framework}. 
arXiv preprint arXiv:2007.14390. 
\url{https://arxiv.org/abs/2007.14390}

\bibitem{fedavg}
McMahan, H. B., Moore, E., Ramage, D., Hampson, S., \& Arcas, B. A. (2017). 
\textit{Communication-Efficient Learning of Deep Networks from Decentralized Data}. 
In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS). 
\url{https://arxiv.org/abs/1602.05629}

\bibitem{fedprox}
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., \& Smith, V. (2020). 
\textit{Federated Optimization in Heterogeneous Networks}. 
Proceedings of Machine Learning and Systems (MLSys). 
\url{https://arxiv.org/abs/1812.06127}

\bibitem{bim}
Kurakin, A., Goodfellow, I., \& Bengio, S. (2016). 
\textit{Adversarial examples in the physical world}. 
arXiv preprint arXiv:1607.02533. 
\url{https://arxiv.org/abs/1607.02533}

\bibitem{blur}
Pillow Documentation. 
\textit{ImageFilter.GaussianBlur}. 
\url{https://pillow.readthedocs.io/en/stable/reference/ImageFilter.html}

\end{thebibliography}

\end{document}
